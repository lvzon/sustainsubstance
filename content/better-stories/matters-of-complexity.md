Title: Entwined: Matters of Complexity
Authors: Levien van Zon
Date: 2024-05-27
Tags: complexity
Slug: matters-of-complexity
Status: draft

Reading time: ca. 10-15 minutes

*This is a draft version (revision 2), the final version will be published on [https://lvzon.substack.com](https://lvzon.substack.com) and here on [https://sustainsubstance.org](http://sustainsubstance.org).*

-----

**By Levien van Zon**

We understand our world through stories.
As I wrote in my [previous article](https://sustainsubstance.org/beyond-optimists-and-pessimists.html), how we see and how we talk about the world matters a lot for the way we understand our surroundings.
This in turn determines which problems we see and how we choose to deal with them.
Stories are much more than just "fiction". 
They are internal models of the world that we share with others through the magical medium of language. 
Stories constitute knowledge, both for us as individuals and as a species.

Many of our day-to-day stories about the world are sloppy and inconsistent.
Yet much of our modern knowledge derives from science. 
Science aims to tell stories that are more formal and more precise, and that can be examined for their accuracy.
When we're looking for better stories, the stories of science are not a bad place to start.

### Welcome to the machine

Since about the 17th century, our collective ways of looking at the world have increasingly been determined by science and technology. 
Especially the concept of "the machine" has historically been very important in shaping our thinking as well as our "doing".
Many early scientists and Enlightenment thinkers such as Thomas Hobbes imagined the world as being similar to a giant machine or clockwork. 
They believed that this [clockwork universe](https://en.wikipedia.org/wiki/Clockwork_universe) could be fully understood and predicted, once the mechanical laws that govern it had been uncovered.
Also life and human society were often talked about in terms of machines. 
For instance, René Descartes famously argued that animals were basically just automatons, biological machines incapable of conscious experience because they lacked a human soul. 

The Industrial Revolution in the 18th and 19th centuries saw the development, study and optimisation of actual human-built machines for an increasing range of tasks. 
This yielded further insights into all kinds of mechanical laws, but also into the role of energy, including the behaviour of heat, electricity, magnetism and light.
Increasingly, nature was seen as something that could and *should* be controlled and improved for the benefit of humans.
The forests of Europe were turned into artificial monocultures, optimised for the maximum yield of wood.
In the 20th century, the efficiency of factories was maximised according to the "rational" principles of Charles Taylor and others. Cities were reimagined as "machines for living and working". 
Much of agriculture was also modelled after the successful ideal of the factory, which was based on mechanisation, standardisation, scale and control. The aim was to replace the seeming chaos of nature by the efficiency of the factory, in order to increase the production and predictability of food, vegetable oil and fibre. This also resulted in driving down their cost.

In many ways these collective attempts to understand and improve the world were an astounding success.
We understand almost unimaginably more about the universe than we did a few centuries ago, and in many countries the quality and span of human life have greatly increased.
But the successes did come at a cost. 
Some of these costs took (and still take) the form of human and non-human suffering, for instance in the former colonies and in industrial production chains, including those of agriculture.
Moreover, while over the past centuries we collectively managed to reduce some problems, we also created many new ones, or made existing problems worse.
One unintended consequence of mechanisation turned out to be climate change. 
Other examples of the darker sides of "progress" include biodiversity loss, soil degradation, water shortages, microbial resistance, a pandemic of chronic metabolic diseases (such as diabetes) and some aspects of social and economic inequality.

If we want to solve and prevent such problems, it is important to figure out why they occur in the first place.
Many "modern" problems were unintended and unexpected side effects of our attempts to improve human wellbeing.
Proponents of technological solutions sometimes argue that these are simply a temporary price that we need to pay for progress.
Further improvements in knowledge and technology will allow us to solve these problems and prevent new ones. 
Is this a realistic expectation, or is it just wishful thinking?

To understand both the success and the limitations of science and technology, it helps to comprehend a little better how science works, and why it is so effective in answering some questions while having trouble with others.
As we shall see, many problems actually have to do with our tendency to use human-built machines as a metaphor for how the world works.
We tend to design machines with a single purpose, and in a way that allows us to take them apart.
Machines are built to be optimised and controlled.
But it turns out that most of our world is very unlike a machine.

### Stories, science and the golden rule

We all use stories to explain our surroundings. 
To find out how accurate our explanations are, we constantly compare our story-based expectations with our actual  observations and experiences.
The way in which science works is not much different, in principle. 
Scientific theories are basically explanatory and predictive stories, and we constantly compare them with observations to see if they are accurate.
However, there is at least one important thing that sets scientific explanations apart from other types of stories.
Regular stories tend to act as a source of certainty. 
If we encounter observations that are inconsistent with our explanatory stories, we tend to resist giving up our stories and we often ignore or explain away the inconsistent observations.
This also happens in science, of course, simply because scientists are humans. 
But science has the underlying, collective rule that, at least in principle, observations are *always* more important than the story used to explain it. 
Philosopher [Michael Strevens](https://www.strevens.org/scientia/) has called this "the golden rule of science".
And this rule is considered sufficiently important that, eventually, it tends to win out against human nature:
If the explanation doesn't match the observation, the story isn't good enough and should be modified.

Observations in science are not always passive.
Often observations involve active and repeatable manipulations of the world, in the form of experiments.
The language of scientific stories is often a "natural" human language such as English. 
When possible, the more precise "non-natural" languages of mathematics and logic are also used. 
Whatever the form, the purpose is the same: to describe a part of the world, in an attempt to
understand how it works. 
This has been especially successful for understanding the non-living physical universe. 
The living world, which includes human societies, has been harder to describe and comprehend. 
We will explore some of the reasons for this. 
For now let's just state that the living world is more complex than the non-living world. 
And this has proven somewhat of a challenge for the tools that science has traditionally used.

In this article (and the next), we will look at so-called *complex systems*. 
We will examine a number of new tools and concepts that have been developed  over the past decades to better understand such systems. 
Collectively, these new tools and concepts are sometimes termed *complexity science.*

The "classical" way to do science is often called *reductionism*. 
The reductionist method basically works by taking complicated things apart.
The parts are then manipulated and studied in isolation. 
In this way, a description of more complicated assemblages is built up from descriptions of its parts. 
This works well, as long as the behaviour of an assemblage is mostly determined by the properties of the parts (or by statistical regularities in the interactions of parts).
For instance, finding out the properties of atoms and simple molecules greatly increased our understanding of how more complicated molecules work. 
It also explained much about the properties of gases, fluids and solids, which are formed from large numbers of interacting atoms or molecules.

Studying parts in a controlled environment allows us to break up the study of complicated things into smaller, more manageable projects.
These smaller studies can also be more easily repeated and checked by others. 
This method is very powerful, and has been extremely successful.
However, it may encounter difficulties when there are *many* interactions between elements. 
When such interactions are significant and are not easy to "aggregate" into neat statistics, 
it becomes hard to explain system behaviour by just studying the interacting parts. 
This is where complexity science can help us, because it is in many respects a "science of interactions". 
It is actually not so much a science in itself, but more an add-on to the various sciences. 
It is a toolkit for thinking and talking about complex systems.

### Interacting elements

So what is a complex system? A *system* is a collection of elements.
These elements need to be interconnected in ways that produce some collective behaviour. 
If a system is *complex*, it usually means that the system has many parts. 
Also, interactions between these parts are important for collective behaviour. 
The word "complex" comes from Latin, and basically means "entwined". 
In a complex system, the parts are hard to separate. 
An example is your body: if you start taking it apart into separate organs or cells, at some point it will stop working well. 
The same applies to most living systems, which is why they are hard to study using the reductionist method. 
You cannot really take them apart without changing the way they work. 
And performing controlled experiments on the interacting parts of a fully functional system isn't easy. 
Reliably repeating such experiments is even harder.

We should point out that complexity isn't a binary category. 
It's not that something is either simple or complex. 
Complexity is a continuum, some things are more complex than others. 
More complexity can result from more parts or interactions, or because interactions are stronger or more varied. 

It is sometimes useful to make a distinction between two kinds of complex systems: *complex physical systems* (CPS) and *complex adaptive systems* (CAS). 
Complex physical systems have many parts that interact, but the parts themselves *don't change much over time*. 
A relatively simple example would be a pile of sand grains. 
A more complex example is the weather.

In a complex *adaptive* system on the other hand, the *parts aren't static*, they can change over time. 
If parts can change, and if they don't all change in the same way, 
individual elements may become different from each other in their properties. 
Complex adaptive systems therefore tend to have *diversity* in their elements. 
And as the name already implies, complex adaptive systems can *adapt* to changing conditions. 
All living systems are complex adaptive systems. 
Examples include biological cells, tissues and organs, organisms, ecosystems and human societies and economies.

### More than the sum of its parts

In both types of complex systems, interactions between the parts can lead to interesting things. 
One of these is *emergence*. This basically means that an assemblage has properties that its parts do not have. 
Think for instance of water, which (in its liquid form) can flow and is wet. 
Water is formed from interacting molecules, but a single water molecule cannot be said to be "wet". 
And even though all water molecules are alike, the [many properties of water](https://en.wikipedia.org/wiki/Properties_of_water) cannot easily be predicted from knowing the properties of a water molecule.

In general, interacting molecules form gases, fluids or solids. 
We can think of these "states of matter" as new *levels of organisation*, which *emerge* from the interactions between molecules. 
A fluid or solid is a "thing" in itself, which has its own properties and follows its own peculiar rules. 
Water and ice are clearly very different, even though they are both made up of the same water molecules. 
The different *emergent properties* of water, ice and water vapour arise because the same molecules interact in different ways.

Emergence is sometimes presented as something that is exotic, hard to study and almost mystical. 
But we are completely surrounded by it, and it determines our daily experience. 
We cannot see molecules interact, so of course we are not used to thinking of the "things" that surround us as emerging from interacting molecules. 
And this is precisely the point: even if we don't know anything about molecules, 
we can still interact with things that are made up of molecules, 
because such emergent levels of organisation have their own emergent properties of "thingness". 
You can simply sit on a chair without being aware of the molecules of iron, nickel and nitrogen that interact to form crystal microstructures that (hopefully) keep the steel frame of the chair together. 
Even if we *are* interested in the finer details of things, a medical specialist can still study and treat heart problems without knowing all of the processes going on in the human body, or all of the molecular components that make up the human heart. 
And an economist can say something about the global market for office chairs without having to study the detailed neural patterns in the brains of all the people that are involved in producing, selling and buying such chairs.
Emergent properties allow us to know things without having to know all of the underlying details.

### Strict laws, shaky laws

Emergence is also mostly compatible with reductionism in science. 
In the 17th century, scientists like Isaac Newton believed that much of the universe was governed by a limited and unchanging set of "laws". 
These laws were presumed to be set by the Creator, and humans could "discover" them through careful experimentation and observation. 
Finding the laws of nature was like unveiling the mind of God.
Over the subsequent centuries, especially physicists proved to be very successful in finding such scientific laws. 
Notably at the subatomic level, many of the rules that govern quantum mechanics seem to be static, fundamental properties of the universe in some way. 
One example is the *Pauli exclusion principle*, a simple rule that prescribes the structure of the periodic table of elements and much of the properties of matter. 
A rule like this has a huge effect on the structure of reality, yet is not entirely clear where it comes from.

Following the successes in physics and chemistry, scientists in other fields also started searching for scientific laws. 
However, the more complex the system that they were studying, the harder it was to find true "natural laws". 
Rules and regularities in the realms of biology, economics and the social sciences bear little resemblance to the "hard" predictive laws of physics. 
They are usually more trends than laws: they tend to depend on context, it is often possible to find exceptions and the rules can suddenly change over time. 

It turns out that even in physics and chemistry, many of the "laws of nature" are not unchanging properties of the universe. 
Rather, they are emergent properties of interacting particles and forces, and they may also depend on context.
As long as the interacting parts (say, molecules) are constant over time, the resulting natural laws are also constant. 
For instance, water freezes at 0°C and boils at 100°C, but this does depend on context. 
An atmospheric pressure other than the "standard" value of 101,325 Pa will shift the boiling and freezing points to higher or lower temperatures. 
Dissolved substances can also do this, and even the speed of heating or cooling can have an effect.
But at least water molecules are all the same, so the effects of external influences are more or less predictable.

However as I already mentioned, discovering laws becomes much more difficult if the interacting parts aren't constant over time. 
In complex adaptive systems, "natural laws" at the level of the system aren't fixed, they can change over time as the interacting parts change. The rules that seem to govern the economy depend on the structure of the economy, and on the usual, average behavioural patterns of people. If these things change, so do the rules, sometimes.
Even worse, a seemingly small change in the underlying parts or interactions can occasionally lead to dramatically different system rules and behaviours. 
Minor interaction details can therefore matter a lot.

In any given study you can only look at a limited set of conditions and interactions. 
Therefore it is often very cumbersome to figure out what's going on in a complex adaptive system. 
It can become almost impossible to predict what will happen if conditions change. 
We can still learn and know things about complex systems, even if their parts are not all identical and constant. 
But it requires a lot of work, and our ability to make generalised statements and predictions is rather limited.

### Predictability problems

This last point is worth emphasising. The behaviour of complex systems usually cannot be predicted very far into the future, even in the unlikely case that all elements and their interactions are known. 
This is not just because elements and interactions may sometimes change. 
The problem also exists in many complex physical systems. 

When there are many strong interactions, every element can end up influencing many other elements, including itself. 
And because interactions take time and this time may vary, it becomes impossible to predict the precise order in which interactions take place. 
This creates an uncertainty about the outcome of interactions, which can rapidly get worse with time. 
This is the main reason why we cannot predict the weather with good accuracy more than a few days ahead.

And it gets worse. We saw that emergent properties allow us to know about and interact with things without having to know all of the underlying details. 
But the inverse isn't true. 
Even if we would have full knowledge about all details of physics and chemistry (which currently isn't the case), this wouldn't necessarily help us to explain and predict what happens at higher, emergent levels of organisation. 
The problem is that what happens in, say, regional politics cannot easily be reduced to physics and chemistry, or even to neuroscience. 
It depends mostly on interactions between, in this case, politicians and other groups in a society. 
It also depends on interactions with other systems (for instance, economics) and other levels of organisation (like
national and international politics and geopolitical power relations).

In my [previous article](https://sustainsubstance.org/beyond-optimists-and-pessimists.html),
I talked about various ways of seeing the world in relation to sustainability. 
The ecomodernist worldview tends to emphasise the potential for large-scale technological solutions, 
based on existing institutions and economic growth. 
In contrast, the antimodernist or neo-romantic worldview tends to promote smaller-scale solutions with a bigger role for nature and local communities.

Neo-romantics often point out the fact that we live in a world that is dominated by complex systems. 
This has consequences for how much we can rely on large-scale control and on stable economic growth. 
Technology and growth require conditions that are more or less stable and predictable. 
As we have seen, predictability is inherently limited in a complex world.

### Promises of "simplexity"

In the 1980s and 1990s, there was a hope that complexity science would allow us to eventually predict and control many complex systems.
Researchers observed that complex behaviour can sometimes arise from very simple interaction rules. 
There are many examples of this in nature, such as the foraging behaviour of ants or the flocking of starlings. 
The expectation was that we would be able to uncover simple rules to explain the behaviour of many other complex systems, including human societies. 
Indeed, there has been much progress in our understanding. 
But the more we learn, the more it seems that the potential for prediction and generalisation is limited. 

The specific details of a system can matter a lot. 
Even if we can find simple rules underlying its behaviour, we still have to *find* those rules for every system. 
And if conditions change, the rules can change as well and our predictions may no longer work.

Yet we would overstate the problem if we conclude that the world is too complex for us to understand, or that no control is possible. 
We certainly shouldn't ignore complexity, but we also shouldn't be overly afraid of it. 
The living world has always been complex, and in recent years we have learned a lot about the ways in which living systems don't just deal with complexity and unpredictability, but in some ways even use it to their advantage.

As we shall see in the next article, there are recurring patterns in the way that evolved, living systems deal with a complex, noisy and sometimes unpredictable world. 
By looking for such patterns and studying why they work well, we can learn about alternative ways of dealing with problems. 
There are alternatives to the industrial approach of maximising efficiency and control, and these alternatives have been
shown by life to work very well in the long term.

-----

Do you want to be notified when future articles in this series are published? Subscribe to my [Substack](https://lvzon.substack.com/), or follow me on [Facebook](https://www.facebook.com/sustainsubstance), [Bluesky](https://bsky.app/profile/lvzon.bsky.social) or [Twitter/X](https://twitter.com/levienvanzon). You can also subscribe to our [Atom-feed](/feeds/all.atom.xml).


### Further reading

Strevens, Michael. *The Knowledge Machine: How Irrationality Created
Modern Science*. W.W. Norton, 2020.     
<https://www.strevens.org/scientia>     
Philosopher Michael Strevens makes the argument that science is special 
mostly due to what he calls its "golden rule": Empirical observations
always have precedence over theoretical or philosophical explanations. 
Moreover, as an institution, science sets strict rules on acceptable behaviour 
and the things that may or may not be discussed in its official communication channels. 
This creates a buffer between public scientific work (which is published in journals)
and private beliefs (which may be discussed and published elsewhere). 
This explains why for instance Isaac Newton could both be one of the great
empirical scientists, and at the same time a highly religious alchemist. 

Holland, John H. *Complexity: A Very Short Introduction*. Oxford
University Press, 2014.     
<https://doi.org/10.1093/actrade/9780199662548.001.0001>

Gershenson, Carlos (2013) 'Facing Complexity: Prediction vs.
Adaptation', in À. Massip-Bonet and A. Bastardas-Boada (eds.)
*Complexity Perspectives on Language, Communication and Society*.
Berlin, Heidelberg: Springer (Understanding Complex Systems), pp.
3--14.     
<https://doi.org/10.1007/978-3-642-32817-6\_2>
([PDF](https://www.researchgate.net/profile/Carlos-Gershenson/publication/285965809_Facing_complexity_Prediction_vs_adaptation/links/591107baa6fdccbfd5a781fd/Facing-complexity-Prediction-vs-adaptation.pdf))

Gershenson, Carlos (2013) 'The Implications of Interactions for Science
and Philosophy', *Foundations of Science*, 18(4), pp. 781--790.     
<https://doi.org/10.1007/s10699-012-9305-8>
([PDF](https://www.academia.edu/download/31338551/ImplicationsInteractions-FoS.pdf))

Waldrop, Mitchell M. *Complexity: The Emerging Science at the Edge of
Order and Chaos*. Simon and Schuster, 1993.     
<https://www.simonandschuster.com/books/Complexity/Mitchell-M-Waldrop/9780671872342>     
This book by Mitchell Waldrop is what got me (and many others)
interested in complexity science. Since its publication, it has been
criticised by some for its narrow focus on the Santa Fe Institute and
its portrayal of complexity researchers as young
rebels fighting against a conservative scientific establishment clinging
on to reductionism (which is not entirely accurate). 
Also, the book is now three decades old, and some of its
ideas on the early hopes of complexity science seem a little
optimistic in hindsight. But *Complexity* still provides an attractive
and readable introduction to many aspects of complex
adaptive systems.

Ball, Philip. *Why Society is a Complex Matter: Meeting Twenty-first Century Challenges with a New Kind of Science*. Springer Berlin Heidelberg, 2012. 
<https://link.springer.com/book/10.1007/978-3-642-29000-8>


Scott, James C. *Seeing Like a State: How Certain Schemes to Improve the Human Condition Have Failed*. New Haven: Yale University Press, 1998.     
<https://yalebooks.yale.edu/book/9780300246759/seeing-like-a-state/>     
The political scientist and anthropologist James C. Scott has pointed out that our goals determine how we structure the world, and also what our blind spots are. He shows how "scientific" foresters of the 19th century and the modernist states of the 20th century simplified the social and natural systems they worked with, in the name of efficiency, rationality and the pursuit of a utopian future. In doing so, they destroyed much of the complex ecological and social fabric required for systems to function well. They failed to appreciate that the seeming chaos of real ecological and social systems actually contributes to their healthy function and long-term resilience.

Ball, Philip. *How Life Works: A User's Guide to the New Biology*. University of Chicago Press, 2023. 
<https://how-life-works.philipball.co.uk>

Merchant, Carolyn. *Autonomous Nature: Problems of Prediction and Control from Ancient Times to the Scientific Revolution*. Routledge, 2016. 
<http://www.bikingbooks.com/html/merchant.html>



-----------

#### Footnotes

[^basiclaws]: *What we usually think of as "natural laws" and properties of things in the world  are to some extent emergent properties of interacting particles and forces.*     
While the description of all natural laws as arising from interactions of 
elements at another level of organisation is simple and attractive, it probably 
isn't accurate. 
Especially at the subatomic level, many of the rules that govern quantum mechanics
seem to be static, fundamental properties of the universe in some way. One important 
example is the *Pauli exclusion principle*, a simple rule that prescribes the 
structure of the periodic table of elements and much of the properties of matter.
It is not clear where such a rule come from.     
Also note that I use quite a broad definition of emergence here, which certainly is not 
generally used or accepted. While it may be very useful, emergence can also be a 
tricky and slippery concept once you try to get into the details---something 
which I shall not do here.

